{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmx = pd.read_csv('net.csv')\n",
    "dfsg = pd.read_csv('net32.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       Idx SeqId AltSeqId    TId Layer  \\\n746    747   255      253  27392     -   \n748    749   260      258  27392     -   \n1064  1065   293      294  27392     -   \n1123  1124   294        -  36946     -   \n1124  1125   294        -  36946     -   \n1443  1444   274        -  36946     -   \n1463  1464   260        -  36946     -   \n1464  1465   260        -  36946     -   \n1466  1467   255        -  36946     -   \n1467  1468   255        -  36946     -   \n\n                                                  Trace Direction  Sub  \\\n746   train.py:625,train.py:557,/home/aditya/GNMTRes...     fprop    0   \n748   train.py:625,train.py:557,/home/aditya/GNMTRes...     fprop    0   \n1064  train.py:625,train.py:557,/home/aditya/GNMTRes...     fprop    0   \n1123  train.py:625,train.py:557,/home/aditya/GNMTRes...     bprop    0   \n1124  train.py:625,train.py:557,/home/aditya/GNMTRes...     bprop    1   \n1443  train.py:625,train.py:557,/home/aditya/GNMTRes...     bprop    0   \n1463  train.py:625,train.py:557,/home/aditya/GNMTRes...     bprop    0   \n1464  train.py:625,train.py:557,/home/aditya/GNMTRes...     bprop    1   \n1466  train.py:625,train.py:557,/home/aditya/GNMTRes...     bprop    0   \n1467  train.py:625,train.py:557,/home/aditya/GNMTRes...     bprop    1   \n\n                   Module      Op  \\\n746   torch.nn.functional  linear   \n748   torch.nn.functional  linear   \n1064  torch.nn.functional  linear   \n1123  torch.nn.functional  linear   \n1124  torch.nn.functional  linear   \n1443               Tensor  matmul   \n1463  torch.nn.functional  linear   \n1464  torch.nn.functional  linear   \n1466  torch.nn.functional  linear   \n1467  torch.nn.functional  linear   \n\n                                            Kernel  \\\n746   volta_fp16_s884gemm_fp16_128x128_ldg8_f2f_tn   \n748   volta_fp16_s884gemm_fp16_128x128_ldg8_f2f_tn   \n1064  volta_fp16_s884gemm_fp16_128x128_ldg8_f2f_tn   \n1123  volta_fp16_s884gemm_fp16_256x128_ldg8_f2f_nt   \n1124  volta_fp16_s884gemm_fp16_256x128_ldg8_f2f_nn   \n1443   volta_fp16_s884gemm_fp16_128x64_ldg8_f2f_nn   \n1463  volta_fp16_s884gemm_fp16_256x128_ldg8_f2f_nt   \n1464  volta_fp16_s884gemm_fp16_128x128_ldg8_f2f_nn   \n1466  volta_fp16_s884gemm_fp16_256x128_ldg8_f2f_nt   \n1467  volta_fp16_s884gemm_fp16_128x128_ldg8_f2f_nn   \n\n                                  Params  Sil(ns) TC  Device  Stream  \\\n746       M=1024,N=(128,38),K=1024,fp16,   131871  1       0       7   \n748       M=1024,N=(128,39),K=1024,fp16,   125600  1       0       7   \n1064     M=37008,N=(38,128),K=1024,fp16,  8257756  1       0       7   \n1123     M=1024,N=(38,128),K=37008,fp16,  7633919  1       0       7   \n1124     M=1024,N=37008,K=(38,128),fp16,  8634107  1       0       7   \n1443  A=(128,38,39,1024),B=(1024,),fp16,   509917  1       0       7   \n1463      M=1024,N=(128,39),K=1024,fp16,   133887  1       0       7   \n1464      M=1024,N=1024,K=(128,39),fp16,   126208  1       0       7   \n1466      M=1024,N=(128,38),K=1024,fp16,   123072  1       0       7   \n1467      M=1024,N=1024,K=(128,38),fp16,   120287  1       0       7   \n\n          Grid    Block         FLOPs      Bytes  \n746     8,38,1  128,1,1  1.020055e+10   15990784  \n748     8,39,1  128,1,1  1.046898e+10   16384000  \n1064  290,38,1  128,1,1  3.686542e+11  227864576  \n1123   4,290,1  256,1,1  3.686542e+11  227864576  \n1124    4,38,1  256,1,1  3.686542e+11  227864576  \n1443    8,1,80  128,1,1  3.884974e+08      81998  \n1463     4,8,5  256,1,1  1.046898e+10   16384000  \n1464    8,39,1  128,1,1  1.046898e+10   16384000  \n1466     4,8,5  256,1,1  1.020055e+10   15990784  \n1467    8,38,1  128,1,1  1.020055e+10   15990784  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Idx</th>\n      <th>SeqId</th>\n      <th>AltSeqId</th>\n      <th>TId</th>\n      <th>Layer</th>\n      <th>Trace</th>\n      <th>Direction</th>\n      <th>Sub</th>\n      <th>Module</th>\n      <th>Op</th>\n      <th>Kernel</th>\n      <th>Params</th>\n      <th>Sil(ns)</th>\n      <th>TC</th>\n      <th>Device</th>\n      <th>Stream</th>\n      <th>Grid</th>\n      <th>Block</th>\n      <th>FLOPs</th>\n      <th>Bytes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>746</th>\n      <td>747</td>\n      <td>255</td>\n      <td>253</td>\n      <td>27392</td>\n      <td>-</td>\n      <td>train.py:625,train.py:557,/home/aditya/GNMTRes...</td>\n      <td>fprop</td>\n      <td>0</td>\n      <td>torch.nn.functional</td>\n      <td>linear</td>\n      <td>volta_fp16_s884gemm_fp16_128x128_ldg8_f2f_tn</td>\n      <td>M=1024,N=(128,38),K=1024,fp16,</td>\n      <td>131871</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7</td>\n      <td>8,38,1</td>\n      <td>128,1,1</td>\n      <td>1.020055e+10</td>\n      <td>15990784</td>\n    </tr>\n    <tr>\n      <th>748</th>\n      <td>749</td>\n      <td>260</td>\n      <td>258</td>\n      <td>27392</td>\n      <td>-</td>\n      <td>train.py:625,train.py:557,/home/aditya/GNMTRes...</td>\n      <td>fprop</td>\n      <td>0</td>\n      <td>torch.nn.functional</td>\n      <td>linear</td>\n      <td>volta_fp16_s884gemm_fp16_128x128_ldg8_f2f_tn</td>\n      <td>M=1024,N=(128,39),K=1024,fp16,</td>\n      <td>125600</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7</td>\n      <td>8,39,1</td>\n      <td>128,1,1</td>\n      <td>1.046898e+10</td>\n      <td>16384000</td>\n    </tr>\n    <tr>\n      <th>1064</th>\n      <td>1065</td>\n      <td>293</td>\n      <td>294</td>\n      <td>27392</td>\n      <td>-</td>\n      <td>train.py:625,train.py:557,/home/aditya/GNMTRes...</td>\n      <td>fprop</td>\n      <td>0</td>\n      <td>torch.nn.functional</td>\n      <td>linear</td>\n      <td>volta_fp16_s884gemm_fp16_128x128_ldg8_f2f_tn</td>\n      <td>M=37008,N=(38,128),K=1024,fp16,</td>\n      <td>8257756</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7</td>\n      <td>290,38,1</td>\n      <td>128,1,1</td>\n      <td>3.686542e+11</td>\n      <td>227864576</td>\n    </tr>\n    <tr>\n      <th>1123</th>\n      <td>1124</td>\n      <td>294</td>\n      <td>-</td>\n      <td>36946</td>\n      <td>-</td>\n      <td>train.py:625,train.py:557,/home/aditya/GNMTRes...</td>\n      <td>bprop</td>\n      <td>0</td>\n      <td>torch.nn.functional</td>\n      <td>linear</td>\n      <td>volta_fp16_s884gemm_fp16_256x128_ldg8_f2f_nt</td>\n      <td>M=1024,N=(38,128),K=37008,fp16,</td>\n      <td>7633919</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7</td>\n      <td>4,290,1</td>\n      <td>256,1,1</td>\n      <td>3.686542e+11</td>\n      <td>227864576</td>\n    </tr>\n    <tr>\n      <th>1124</th>\n      <td>1125</td>\n      <td>294</td>\n      <td>-</td>\n      <td>36946</td>\n      <td>-</td>\n      <td>train.py:625,train.py:557,/home/aditya/GNMTRes...</td>\n      <td>bprop</td>\n      <td>1</td>\n      <td>torch.nn.functional</td>\n      <td>linear</td>\n      <td>volta_fp16_s884gemm_fp16_256x128_ldg8_f2f_nn</td>\n      <td>M=1024,N=37008,K=(38,128),fp16,</td>\n      <td>8634107</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7</td>\n      <td>4,38,1</td>\n      <td>256,1,1</td>\n      <td>3.686542e+11</td>\n      <td>227864576</td>\n    </tr>\n    <tr>\n      <th>1443</th>\n      <td>1444</td>\n      <td>274</td>\n      <td>-</td>\n      <td>36946</td>\n      <td>-</td>\n      <td>train.py:625,train.py:557,/home/aditya/GNMTRes...</td>\n      <td>bprop</td>\n      <td>0</td>\n      <td>Tensor</td>\n      <td>matmul</td>\n      <td>volta_fp16_s884gemm_fp16_128x64_ldg8_f2f_nn</td>\n      <td>A=(128,38,39,1024),B=(1024,),fp16,</td>\n      <td>509917</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7</td>\n      <td>8,1,80</td>\n      <td>128,1,1</td>\n      <td>3.884974e+08</td>\n      <td>81998</td>\n    </tr>\n    <tr>\n      <th>1463</th>\n      <td>1464</td>\n      <td>260</td>\n      <td>-</td>\n      <td>36946</td>\n      <td>-</td>\n      <td>train.py:625,train.py:557,/home/aditya/GNMTRes...</td>\n      <td>bprop</td>\n      <td>0</td>\n      <td>torch.nn.functional</td>\n      <td>linear</td>\n      <td>volta_fp16_s884gemm_fp16_256x128_ldg8_f2f_nt</td>\n      <td>M=1024,N=(128,39),K=1024,fp16,</td>\n      <td>133887</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7</td>\n      <td>4,8,5</td>\n      <td>256,1,1</td>\n      <td>1.046898e+10</td>\n      <td>16384000</td>\n    </tr>\n    <tr>\n      <th>1464</th>\n      <td>1465</td>\n      <td>260</td>\n      <td>-</td>\n      <td>36946</td>\n      <td>-</td>\n      <td>train.py:625,train.py:557,/home/aditya/GNMTRes...</td>\n      <td>bprop</td>\n      <td>1</td>\n      <td>torch.nn.functional</td>\n      <td>linear</td>\n      <td>volta_fp16_s884gemm_fp16_128x128_ldg8_f2f_nn</td>\n      <td>M=1024,N=1024,K=(128,39),fp16,</td>\n      <td>126208</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7</td>\n      <td>8,39,1</td>\n      <td>128,1,1</td>\n      <td>1.046898e+10</td>\n      <td>16384000</td>\n    </tr>\n    <tr>\n      <th>1466</th>\n      <td>1467</td>\n      <td>255</td>\n      <td>-</td>\n      <td>36946</td>\n      <td>-</td>\n      <td>train.py:625,train.py:557,/home/aditya/GNMTRes...</td>\n      <td>bprop</td>\n      <td>0</td>\n      <td>torch.nn.functional</td>\n      <td>linear</td>\n      <td>volta_fp16_s884gemm_fp16_256x128_ldg8_f2f_nt</td>\n      <td>M=1024,N=(128,38),K=1024,fp16,</td>\n      <td>123072</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7</td>\n      <td>4,8,5</td>\n      <td>256,1,1</td>\n      <td>1.020055e+10</td>\n      <td>15990784</td>\n    </tr>\n    <tr>\n      <th>1467</th>\n      <td>1468</td>\n      <td>255</td>\n      <td>-</td>\n      <td>36946</td>\n      <td>-</td>\n      <td>train.py:625,train.py:557,/home/aditya/GNMTRes...</td>\n      <td>bprop</td>\n      <td>1</td>\n      <td>torch.nn.functional</td>\n      <td>linear</td>\n      <td>volta_fp16_s884gemm_fp16_128x128_ldg8_f2f_nn</td>\n      <td>M=1024,N=1024,K=(128,38),fp16,</td>\n      <td>120287</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7</td>\n      <td>8,38,1</td>\n      <td>128,1,1</td>\n      <td>1.020055e+10</td>\n      <td>15990784</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "dfmx[dfmx['TC'] == '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "172351874"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "dfmx[dfmx['Kernel'].str.contains('volta')][dfmx['Module'] == 'LSTM']['Sil(ns)'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "246347772\n290500537\n"
    }
   ],
   "source": [
    "print(dfmx['Sil(ns)'].sum() - 25796624)\n",
    "print(dfsg['Sil(ns)'].sum() - 66996339)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594826272821",
   "display_name": "Python 3.6.9 64-bit ('kaggle': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}